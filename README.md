# SwiftSupply Logistics â€“ Azure Data Platform

## ğŸš€ Project Overview
Build an **end-to-end Azure Data Platform** for SwiftSupply Logistics (fictional company) to handle **batch and streaming data** from retail warehouses, with **Synapse Analytics integration** for enterprise-scale reporting.

---

## ğŸ› ï¸ Technologies Used
- **Azure Data Lake Storage (ADLS)**
- **Azure Databricks (PySpark, Delta Lake)**
- **Azure Data Factory (ADF)**
- **Azure Synapse Analytics** (Dedicated SQL Pool, Pipelines)
- **Azure Key Vault** (Secrets Management)
- **Python, PySpark**
- **AWS S3** (optional source storage)

---

## ğŸ“– Introduction
SwiftSupply Logistics Pvt Ltd operates a retail & e-commerce supply chain with **5 warehouses across India** (Mumbai, Delhi, Bangalore, Chennai, Hyderabad).

The company faced:
- ğŸ“‰ Stock shortages due to poor demand forecasting  
- ğŸšš Customer complaints because of late shipments  
- âŒ No unified view of data â€” fragmented across Excel, SQL Server, IoT devices  

This project builds a **modern Azure Data Platform** that ingests, processes, curates, and warehouses batch and streaming data, enabling **real-time supply chain insights** through **Synapse Analytics**.

---

## ğŸ—ï¸ Architecture
High-level architecture includes:
- **Ingestion** (S3 + IoT)  
- **Storage** (ADLS Bronze, Silver, Gold)  
- **Processing** (Databricks)  
- **Orchestration** (ADF)  
- **Warehousing** (Synapse Dedicated SQL Pool)  
- **Visualization** (Power BI via Gold layer & Synapse)  

---

## ğŸ“‚ Storage Design (ADLS Containers)

archive/
â”œâ”€â”€ batch/{yyyyMMdd}/ # Archived batch CSV files from S3
â””â”€â”€ streaming/{yyyyMMdd}/ # Archived streaming JSON files from IoT

bronze/
â”œâ”€â”€ batch/inventory/ # Raw inventory CSVs
â”œâ”€â”€ batch/salesorder/ # Raw sales order CSVs
â”œâ”€â”€ streaming/landing_shipments/ # Landing zone for shipment JSON events
â””â”€â”€ streaming/shipments/ # Bronze Delta tables (raw streaming data)

silver/
â”œâ”€â”€ batch/inventory/ # Cleaned inventory Delta tables
â”œâ”€â”€ batch/salesorder/ # Cleaned sales order Delta tables
â””â”€â”€ streaming/shipments/ # Processed shipment Delta tables

gold/
â”œâ”€â”€ delta_tables/ # Curated Gold Delta tables (Databricks SQL)
â”œâ”€â”€ parquet/ # Exported Parquet files (for Synapse COPY INTO)
â””â”€â”€ _success/ # Marker file to trigger Synapse pipeline

## ğŸ“Š Data Sources
**Batch (CSV from S3)**
- `inventory`  
- `sales orders`  

**Streaming (IoT JSON from Simulator)**
- `shipments` (in-transit events)  

---

## ğŸ”„ Pipelines & Processing

### A. Batch Pipeline (ADF)
- Triggered daily  
- Validates and ingests CSVs â†’ Bronze  
- Orchestrates Databricks notebooks  
- Archives original files  

### B. Streaming Pipeline (Databricks)
- Autoloader notebooks clean and transform JSON â†’ Silver  

### C. Transformation & Gold Tables (Databricks)
- Batch + streaming joins  
- Creates **7 curated Gold Delta tables**  
- Exports each Gold table as **Parquet** in ADLS `gold/parquet/`  
- Writes `_success.txt` marker when all files are ready  

**Gold Tables**
- `dim_product` (Product details)  
- `dim_warehouse` (Warehouse master)  
- `fact_inventory_daily` (Daily stock levels)  
- `fact_sales_daily` (Sales transactions)  
- `fact_shipments` (Shipment events)  
- `supplychain_snapshot` (In-transit & delayed shipments)  
- `alerts_shipments` (Delayed shipment alerts)  

### D. Synapse Analytics Integration
- **Dedicated SQL Pool** created with `gold` schema  
- Stored procedure with **COPY INTO** loads Parquet files from ADLS â†’ Synapse tables  
- **Synapse Pipeline** monitors `gold/_success/` with **event trigger**  
- On success, pipeline executes stored procedure to refresh warehouse tables  
- **Power BI connects directly to Synapse** for reporting  

---

## âš™ï¸ Business Rules
- **Low Stock Rule**: `< 50` units = â€œLow Stockâ€  
- **Shipment Delay Alerts**: status = â€œDelayedâ€ â†’ insert into alerts  
- **Retention**: keep only last 90 days in curated zone  

---

## ğŸ” Security
- **Azure Key Vault**: credentials for ADLS, Databricks, Synapse  
- **RBAC**:  
  - Bronze â†’ ingestion team  
  - Silver â†’ data engineers  
  - Gold + Synapse â†’ BI team  

---

## ğŸ“¡ Monitoring
- **ADF**: pipeline alerts  
- **Databricks**: streaming job monitoring via UI & logs  
- **Synapse**: pipeline execution logs & SQL monitoring  

---

## ğŸ“ˆ Power BI Dashboard
The Gold tables in **Synapse** are consumed in Power BI to deliver:
- ğŸ“¦ Inventory stock snapshot  
- ğŸšš Real-time shipment tracking  
- âš ï¸ Delay alerts & low stock indicators  

---

## ğŸ“ Repository Structure

swift-supply-azure-data-platform/
â”‚â”€â”€ README.md
â”‚â”€â”€ docs/
â”‚ â”œâ”€â”€ SwiftSupply_Azure_Data_Platform.docx
â”‚ â”œâ”€â”€ architecture_diagram.png
â”‚ â””â”€â”€ PowerBI_dashboard.png
â”‚â”€â”€ data/
â”‚ â”œâ”€â”€ inventory_sample.csv
â”‚ â”œâ”€â”€ salesorder_sample.csv
â”‚ â””â”€â”€ shipments_sample.json
â”‚â”€â”€ notebooks/
â”‚ â”œâ”€â”€ SSL-batch-bronze-to-silver.py
â”‚ â”œâ”€â”€ SSL-silver-gold.py
â”‚ â”œâ”€â”€ SSL-streaming-bronze-silver.py
â”‚ â””â”€â”€ SSL-streaming-source-bronze.py
â”‚â”€â”€ pipelines/
â”‚ â”œâ”€â”€ PL_SwiftSupply_DataIngestion.json
â”‚â”€â”€ scripts/
â”‚ â””â”€â”€ Source_files_generator.py
â”‚â”€â”€ configs/
â”‚ â”œâ”€â”€ keyvault_config.json
â”‚ â””â”€â”€ adls_paths.md
â””â”€â”€ powerbi/
â””â”€â”€ supplychain_dashboard.pbix


---

## ğŸ› ï¸ Setup Guide
1. Deploy **ADLS Gen2 containers** (archive, bronze, silver, gold)  
2. Configure **Azure Key Vault** with credentials  
3. Deploy **ADF pipelines**  
4. Upload sample files to S3  
5. Run shipment simulator  
6. Trigger ADF pipeline â†’ Bronze â†’ Silver  
7. Databricks notebooks â†’ Gold (Delta + Parquet)  
8. `_success.txt` triggers **Synapse Pipeline**  
9. Stored procedure loads data into **Synapse SQL Pool**  
10. Connect **Power BI to Synapse**  

---

## ğŸ”® Next Enhancements
- ğŸ¤– Machine Learning demand forecasting with **Azure ML**  
- âœ… Data Quality Monitoring with **Delta Live Tables** or **Great Expectations**  
- ğŸ”„ CI/CD pipelines for **ADF, Databricks, and Synapse** 
